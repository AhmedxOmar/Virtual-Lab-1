
# 2. Entropy & Information Theory


## üìò What is Entropy?
Entropy measures the average amount of information produced by a source of data. It is a key concept in understanding compression efficiency.

## üìê Mathematical Definition
If a source has \( n \) possible symbols with probabilities \( P_1, P_2, ..., P_n \), the entropy \( H \) is:

\[
H = -\sum_{i=1}^{n} P_i \log_2(P_i)
\]

## üßÆ Example Calculation
Suppose we have probabilities for symbols A, B, and C:
- P(A) = 0.5
- P(B) = 0.3
- P(C) = 0.2

Then:
\[
H = -(0.5\log_2 0.5 + 0.3\log_2 0.3 + 0.2\log_2 0.2) \approx 1.485\text{ bits}
\]

## üêç Python Code for Entropy
```python
import numpy as np
from collections import Counter

# Sample data (a string of characters)
data = 'AAABBCCCCDDDDD'

# Count the frequency of each unique character in the string
frequencies = Counter(data)  # Counter creates a dictionary of character counts

# Calculate the total number of characters in the data
total = sum(frequencies.values())  # This is the sum of all character frequencies

# Calculate the probabilities of each character in the data
probs = [freq / total for freq in frequencies.values()]  # Probability = Frequency / Total number of characters

# Entropy calculation using the formula: H(X) = -Œ£(p(x) * log2(p(x)))
entropy = -sum([p * np.log2(p) for p in probs])  # Sum the probabilities multiplied by their log base 2

# Print the calculated entropy (measured in bits)
print(f"Entropy: {entropy:.4f} bits")  # Display the result with 4 decimal places

```

## üß† MATLAB Code for Entropy
```matlab
% Sample data (a string of characters)
data = 'AAABBCCCCDDDDD';

% Ensure the input data is a valid string (non-empty and non-whitespace)
if isempty(data) || all(isspace(data))
    error('Input data must be a non-empty string.');
end

% Get the unique characters in the data
values = unique(data);  % `unique` returns the distinct characters in the string

% Calculate the total number of characters in the data
n = length(data);  % This is the total length of the data string

% Initialize entropy to 0
entropy = 0;

% Loop through each unique value (character) in the data
for i = 1:length(values)
    % Calculate the probability of the current character (p)
    p = sum(data == values(i)) / n;  % The probability is the count of the character divided by the total
    % Calculate the contribution to the entropy from this character and add it to the total
    if p > 0  % To avoid log2(0), check that the probability is non-zero
        entropy = entropy - p * log2(p);  % Shannon's entropy formula: -p * log2(p)
    end
end

% Print the calculated entropy value
fprintf('Entropy: %.4f bits\n', entropy);  % Display the entropy with 4 decimal places

```

## üñºÔ∏è Image Example
![Entropy Graph](photo/Entropy&Information.png)

